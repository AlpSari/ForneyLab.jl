{
 "metadata": {
  "language": "Julia",
  "name": "naive_variational_message_passing",
  "signature": "sha256:a4bbed390f8d73e5f8895f0377e078d22a7b8f80e57c16765a5c924804d73ead"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Naive variational message passing demo\n===\n\nThe variational method is used to approximate an intractable posterior distribution when hidden variables are present. Variational approximation can be viewed as the minimization of a variational free energy function. This free energy measures the dissimilarity between the intractable true posterior and a more friendly approximate distribution. When the Kullback Leibler divergence is used as dissimilariy measure, the approximate distribution lives under the true posterior and attempts to fit the local properties of the posterior distribution.\n\nVariational methods often involve complicated and extensive derivations in order to execute the free energy minimizations. Variational message passing (VMP) eases our derivation troubles by expressing the variational algorithm in terms of local update rules, using the factorization of the posterior. In his 2007 article, Dauwels gives a generic introduction to VMP on a factor graph. We will implement this VMP approach for the case of a univariate Gaussian posterior. We wish to estimate the latent mean and variance of this Gaussian using VMP.\n\nThe corresponding factor graph is shown below, where $q(m)$, $q(s)$ and $q(y)$ represent the approximated marginals for the mean, variance and sample respectively. Variational messages $v(.)$ are passed forward and backward, and the approximate marginals $q(.)$ are calculated as $q(.)=\\overrightarrow{v}(.) \\times \\overleftarrow{v}(.)$. We observe the samples `y` and wish to estimate the posterior distribution over the mean and variance given these samples.\n\n\n```\n [s_0]---------->[=]---------->[=]--->    -->[s_N]\n                      |             |   etc...\n [m_0]-->[=]---------->[=]--------->      -->[m_N]\n          q(m)|   q(s)|     |       |\n              -->[N]<--     -->[N]<--\n                  |             |\n                  v q(y)        v\n                [y_1]         [y_2]\n```\n\nWe start by building this graph."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "using(ForneyLab)\n\n# Initial settings\nn_samples = 10 # Number of observed samples\nn_its = 50 # Number of vmp iterations\ntrue_mean = 5.0\ntrue_variance = 2.0\ny_observations = sqrt(true_variance)*randn(n_samples)+true_mean # y observation buffer\n\n# Pre-assign arrays for later reference\ng_nodes = Array(GaussianNode, n_samples)\nm_eq_nodes = Array(EqualityNode, n_samples)\ns_eq_nodes = Array(EqualityNode, n_samples)\ny_nodes = Array(TerminalNode, n_samples)\n\n# Build graph\nfor section=1:n_samples\n    g_nodes[section] = GaussianNode()\n    m_eq_nodes[section] = EqualityNode() # Equality node chain for mean\n    s_eq_nodes[section] = EqualityNode() # Equality node chain for variance\n    y_nodes[section] = TerminalNode(y_observations[section]) # Observed y values are stored in terminal node values\n    Edge(g_nodes[section].out, y_nodes[section].out, GaussianDistribution, Float64)\n    Edge(m_eq_nodes[section].interfaces[3], g_nodes[section].mean)\n    Edge(s_eq_nodes[section].interfaces[3], g_nodes[section].variance, InverseGammaDistribution)\n    if section > 1 # Connect sections\n        Edge(m_eq_nodes[section-1].interfaces[2], m_eq_nodes[section].interfaces[1])\n        Edge(s_eq_nodes[section-1].interfaces[2], s_eq_nodes[section].interfaces[1], InverseGammaDistribution)\n    end\nend\n# Attach beginning and end nodes\nm_0 = TerminalNode(GaussianDistribution(m=0.0, V=100.0)) # Prior\ns_0 = TerminalNode(InverseGammaDistribution(a=0.01, b=0.01)) # Prior\nm_N = TerminalNode(uninformative(GaussianDistribution)) # Neutral 'one' message\ns_N = TerminalNode(uninformative(InverseGammaDistribution)) # Neutral 'one' message\nEdge(m_0.out, m_eq_nodes[1].interfaces[1])\nEdge(s_0.out, s_eq_nodes[1].interfaces[1], InverseGammaDistribution)\nEdge(m_eq_nodes[end].interfaces[2], m_N.out)\nEdge(s_eq_nodes[end].interfaces[2], s_N.out, InverseGammaDistribution);",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Next we need to specify the factorization, an update schedule and preset the uninformative marginals. This sounds like a lot of work, but it can be accomlished automatically."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "graph = getCurrentGraph()\n\n# Perform mean field factorization\nfactorizeMeanField!(graph)\n\n# Initialize schedules\nfor subgraph in graph.factorization\n    generateSchedule!(subgraph, graph)\nend\n\n# Preset uninformative marginals\nsetUninformativeMarginals!(graph);",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Now we can iteratively execute these schedules and inspect the results. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# The mean field factorization factorizes the graphs in a random order, so we need to name them\nmean_subgraph = getSubgraph(g_nodes[1].mean.edge) # Gets the subgraph for the mean equality chain\nvar_subgraph = getSubgraph(g_nodes[1].variance.edge) # Gets the subgraph for the variance equality chain\n\n# Perform vmp updates\nfor iter = 1:n_its\n    # q(y) update\n    for y_node in y_nodes\n        y_subgraph = getSubgraph(y_node.out.edge)\n        executeSchedule(y_subgraph)\n    end\n    # q(m) update\n    executeSchedule(mean_subgraph)\n    # q(s) update\n    executeSchedule(var_subgraph)\nend\n# Request the final outcome at the end of the equality chains\nm_out = calculateMessage!(m_N.out.partner)\ns_out = calculateMessage!(s_N.out.partner)\nensureMVParametrization!(m_out.payload)\n\n# Inspect the results\nprintln(\"True mean: $(true_mean)\")\nprintln(\"True variance: $(true_variance)\")\nprintln(\"Number of samples: $(length(y_observations))\")\nprintln(\"Sample mean: $(mean(y_observations))\")\nprintln(\"Sample variance: $(var(y_observations))\")\nprintln(\"\\n----- Estimation after $(n_its) VMP updates -----\")\nprintln(\"Mean estimate: $(mean(m_out.payload)[1]), with variance $(var(m_out.payload)[1, 1])\")\nprintln(\"Variance estimate: $(mean(s_out.payload)), with variance $(var(s_out.payload))\")",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "True mean: 5.0\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "True variance: 2.0\nNumber of samples: 10\nSample mean: 5.569405856201494\nSample variance: 2.0521279736562104"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\n\n----- Estimation after 50 VMP updates -----\nMean estimate: 5.565906140000575, with variance 0.06283823250237473\nVariance estimate: 2.383745983331132, with variance 1.8877890076569122\n"
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    }
   ],
   "metadata": {}
  }
 ]
}