{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:3d807505e55d0651d54a352ec8c7d7f4302f83c4006c697181d462492474fb81"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Nested model comparison with the Savage-Dickey ratio\n",
      "\n",
      "The Savage-Dickey ratio for model comparison (Dickey, 1971; Penny and Ridgway, 2013) is used to as a way to calculate the Bayes factor for nested models. In this demo we first derive the Savage-Dickey ratio, after which we illustrate its usage in ForneyLab.\n",
      "\n",
      "The quality of a model $M_1$ with respect to measured data $D$ is given by Bayes rule:\n",
      "\n",
      "\\begin{equation*}\n",
      "    p(M_1|D) = \\frac{p(D|M_1)p(M_1)}{p(D)}\\,.\n",
      "\\end{equation*}\n",
      "\n",
      "It is hard to calculate this probability directly, because evaluation of $p(D)$ required integration over all possible models. In practice we want to compare the performance of $M_1$ with another model $M_2$, in which case it suffices to evaluate their relative performance on the data:\n",
      "\n",
      "\\begin{equation}\n",
      "    \\frac{p(M_1|D)}{p(M_2|D)} = \\frac{ \\frac{p(D|M_1)p(M_1)}{p(D)} }{ \\frac{p(D|M_2)p(M_2)}{p(D)} } = \\frac{p(D|M_1)}{p(D|M_2)} \\times\\frac{p(M_1)}{p(M_2)} = BF_{12} \\times \\frac{p(M_1)}{p(M_2)}\\,,\n",
      "\\end{equation}\n",
      "\n",
      "where the first term $BF_{12}$ is defined as the Bayes factor. The Bayes factor expresses the likelihood ratio of two models, indicating which model best fits the data. The second term is the prior belief ratio over our models, which is set to $1$ when we have no prior preference (a preference independent of the data) of one model over the other.\n",
      "\n",
      "## Expressing the Bayes factor as the Savage-Dickey ratio\n",
      "\n",
      "When $M_1$ is nested in $M_2$, we can write the Bayes factor as the Savage-Dickey ratio, which we derive below. Nested here means that the parameters $\\{\\theta_1\\}$ that describe $M_1$ are a subset of the parameters $\\{\\theta_1, \\theta_2\\}$ that decribe $M_2$. Therefore the likelihoods for these models compare as\n",
      "\n",
      "\\begin{equation}\n",
      "    p(D|\\theta_1, M_1) = p(D|\\theta_1, \\theta_2=0, M_2)\\,.\n",
      "\\end{equation}\n",
      "\n",
      "It is important to understand this reasoning, because it allows us to write the $p(D|M_1)$ term in the Bayes factor as:\n",
      "\n",
      "\\begin{align*}\n",
      "    p(D|M_1) &= \\int p(D|\\theta_1, M_1) \\times p(\\theta_1|M_1) \\operatorname{d}\\!\\theta_1\\\\\n",
      "    &= \\int p(D|\\theta_1, \\theta_2=0, M_2) \\times p(\\theta_1|M_1) \\operatorname{d}\\!\\theta_1\\\\\n",
      "    &= \\int p(D|\\theta_1, \\theta_2=0, M_2) \\times p(\\theta_1|M_2) \\operatorname{d}\\!\\theta_1\\,,\n",
      "\\end{align*}\n",
      "\n",
      "where we set the prior over $\\theta_1$ the same in both models: $p(\\theta_1|M_1) = p(\\theta_1|M_2)$. We integrate and apply Bayes rule a second time:\n",
      "\n",
      "\\begin{align*}\n",
      "    p(D|M_1) &= p(D|\\theta_2=0, M_2)\\\\\n",
      "    &= \\frac{p(\\theta_2=0|D, M_2)p(D|M_2)}{p(\\theta_2=0|M_2)}\\,.\n",
      "\\end{align*}\n",
      "\n",
      "When we substitute this expression back into the Bayes factor, we see something interesting:\n",
      "\n",
      "\\begin{align*}\n",
      "    \\frac{p(D|M_1)}{p(D|M_2)} = \\frac{p(\\theta_2=0|D, M_2)}{p(\\theta_2=0|M_2)}\\,,\n",
      "\\end{align*}\n",
      "\n",
      "which is known as the Savage-Dickey ratio.\n",
      "\n",
      "## Using the Savage-Dickey ratio in practice\n",
      "\n",
      "The Savage-Dickey ratio is very useful in practice. It says that when we have a nested model where we set the same priors in both models for the overlapping parameters, then we can calculate the Bayes factor by assessing how much the data tells us about the necessity of the additional variables. For example, if the provided data give us reason to believe that the additional parameters in the more complicated model are required to explain the data better:\n",
      "\n",
      "\\begin{equation}\n",
      "    p(\\theta_2=0|D, M_2) < p(\\theta_2=0|M_2)\\,,\n",
      "\\end{equation}\n",
      "\n",
      "then we should favour the more complicated model. Because of numerical stability and intuitive reasons it is more practical to compare the logarithms of the probabilities and communicate the difference in evidence for the two models in terms of decibels:\n",
      "\n",
      "\\begin{equation}\n",
      "    10 \\operatorname{log} BF_{12} = 10 \\operatorname{log} p(\\theta_2=0|D, M_2) - 10 \\operatorname{log}p(\\theta_2=0|M_2)\n",
      "\\end{equation}\n",
      "\n",
      "We examine a practical use case by estimating the parameters for a Gaussian random walk model with step precision $\\gamma$ and drift $m$. We compare a model $M_d$ that incorporates the drift with a simpler nested model $M_s$ that ignores the drift. The factor graphs for both models are shown below:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}